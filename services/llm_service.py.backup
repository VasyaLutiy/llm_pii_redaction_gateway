# services/llm_service.py

import logging
import time
import os
import uuid
from typing import Dict, Any
from llm_pii_proxy.core.models import ChatRequest, ChatResponse
from llm_pii_proxy.core.exceptions import PIIProcessingError, LLMProviderError
from llm_pii_proxy.providers.azure_provider import AzureOpenAIProvider
from llm_pii_proxy.security.pii_gateway import AsyncPIISecurityGateway

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä
logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self, llm_provider: AzureOpenAIProvider, pii_gateway: AsyncPIISecurityGateway):
        self.llm_provider = llm_provider
        self.pii_gateway = pii_gateway
        self.debug_mode = os.getenv('PII_PROXY_DEBUG', 'false').lower() == 'true'

    async def process_chat_request(self, request: ChatRequest) -> ChatResponse:
        request_id = f"req_{int(time.time() * 1000)}"  # –ü—Ä–æ—Å—Ç–æ–π ID –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º session_id –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        session_id = request.session_id or uuid.uuid4().hex
        
        logger.info(f"üöÄ [{request_id}] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É chat request", extra={
            "request_id": request_id,
            "session_id": session_id,
            "model": request.model,
            "messages_count": len(request.messages)
        })
        
        try:
            # 1. –í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ê–ï–ú PII –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–ï –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø
            logger.warning(f"‚ö†Ô∏è [{request_id}] PII –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è tool calling!")
            masked_messages = request.messages  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            total_pii_count = 0

            masked_request = request.model_copy()
            masked_request.messages = masked_messages

            # 2. Call LLM provider with masked request
            logger.info(f"üåê [{request_id}] –≠—Ç–∞–ø 2: –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –≤–Ω–µ—à–Ω–µ–π LLM...")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –∏–º–µ–Ω–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM
            if self.debug_mode:
                logger.debug(f"üì§ [{request_id}] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM —Å–ª–µ–¥—É—é—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:")
                for i, msg in enumerate(masked_request.messages):
                    logger.debug(f"    {i+1}. {msg.role}: {msg.content}")
            
            start_time = time.time()
            response = await self.llm_provider.create_chat_completion(masked_request)
            llm_duration = time.time() - start_time

            # 3. –í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ê–ï–ú –î–ï–ú–ê–°–ö–ò–†–û–í–ê–ù–ò–ï
            logger.warning(f"‚ö†Ô∏è [{request_id}] –î–µ–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è tool calling!")
            # –û—Ç–≤–µ—Ç –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å, –±–µ–∑ –¥–µ–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è

            # –ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç Azure, –µ—Å–ª–∏ model –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 'gpt-4.1', –ø–æ–¥–º–µ–Ω—è–µ–º –Ω–∞ 'gpt-4.1' –¥–ª—è Cursor
            if hasattr(response, 'model') and isinstance(response.model, str) and response.model.startswith('gpt-4.1'):
                response.model = 'gpt-4.1'

            logger.info(f"üéâ [{request_id}] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!", extra={
                "request_id": request_id,
                "total_pii_found": total_pii_count,
                "llm_duration_ms": round(llm_duration * 1000, 2)
            })

            return response
            
        except Exception as e:
            logger.error(f"‚ùå [{request_id}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}", extra={
                "request_id": request_id,
                "error": str(e),
                "error_type": type(e).__name__
            })
            # –ü–µ—Ä–µ–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫–∞–∫ –µ—Å—Ç—å
            if isinstance(e, (PIIProcessingError, LLMProviderError)):
                raise
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º
            raise PIIProcessingError(f"LLMService error: {str(e)}") 

    async def process_chat_request_stream(self, request: ChatRequest):
        """
        –ê–Ω–∞–ª–æ–≥ process_chat_request, –Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç async-–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä ChatResponse-—á–∞–Ω–∫–æ–≤ –¥–ª—è stream-—Ä–µ–∂–∏–º–∞.
        """
        request_id = f"req_{int(time.time() * 1000)}"
        session_id = request.session_id or uuid.uuid4().hex
        logger.info(f"üöÄ [STREAM {request_id}] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É chat request (stream)", extra={
            "request_id": request_id,
            "session_id": session_id,
            "model": request.model,
            "messages_count": len(request.messages)
        })
        try:
            # 1. –í–†–ï–ú–ï–ù–ù–û –û–¢–ö–õ–Æ–ß–ê–ï–ú PII –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–ï –í –°–¢–†–ò–ú–ò–ù–ì–ï
            logger.warning(f"‚ö†Ô∏è [STREAM {request_id}] PII –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è!")
            masked_request = request.model_copy()
            masked_request.messages = request.messages

            # 2. Call LLM provider stream (–±–µ–∑ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è)
            async for chunk in self.llm_provider.create_chat_completion_stream(masked_request):
                # 3. –û—Ç–¥–∞–µ–º chunk –∫–∞–∫ –µ—Å—Ç—å (–±–µ–∑ –¥–µ–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è)
                yield chunk
        except Exception as e:
            logger.error(f"‚ùå [STREAM {request_id}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ stream-–∑–∞–ø—Ä–æ—Å–∞: {str(e)}", extra={
                "request_id": request_id,
                "error": str(e),
                "error_type": type(e).__name__
            })
            raise 