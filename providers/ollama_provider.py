# providers/ollama_provider.py

import logging
import time
import uuid
from typing import AsyncIterator
import httpx
from llm_pii_proxy.core.models import ChatRequest, ChatResponse
from llm_pii_proxy.core.exceptions import LLMProviderError
from llm_pii_proxy.core.interfaces import LLMProvider

logger = logging.getLogger(__name__)

class OllamaProvider(LLMProvider):
    """
    Ollama provider –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏—Ç–≤–æ—Ä—è–µ—Ç—Å—è Azure OpenAI –¥–ª—è –æ–±–º–∞–Ω–∞ Cursor üïµÔ∏è‚Äç‚ôÇÔ∏è
    """
    
    def __init__(self):
        self.base_url = "http://192.168.0.182:11434/v1"
        self.model_name = "qwen2.5:32b-instruct"  # –ò–ª–∏ –ª—é–±–∞—è –¥—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å –≤ Ollama
        self.client = httpx.AsyncClient(timeout=30.0)
        
        logger.info("ü¶ô Ollama Provider –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–ø—Ä–∏—Ç–≤–æ—Ä—è–µ—Ç—Å—è Azure OpenAI)")
        logger.info(f"üîó Base URL: {self.base_url}")
        logger.info(f"ü§ñ Model: {self.model_name}")
    
    def _model_supports_tools(self) -> bool:
        """
        –ü—Ä–∏—Ç–≤–æ—Ä—è–µ–º—Å—è —á—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º tools –∫–∞–∫ Azure OpenAI
        """
        return True
    
    @property
    def provider_name(self) -> str:
        return "ollama-fake-azure"
    
    async def create_chat_completion(self, request: ChatRequest) -> ChatResponse:
        start_time = time.time()
        
        logger.info("ü¶ô –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama (–ø—Ä–∏—Ç–≤–æ—Ä—è–µ–º—Å—è Azure)", extra={
            "model": self.model_name,
            "messages_count": len(request.messages),
            "session_id": request.session_id
        })
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è Ollama
            ollama_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in request.messages
            ]
            
            payload = {
                "model": self.model_name,
                "messages": ollama_messages,
                "temperature": request.temperature or 0.7,
                "max_tokens": request.max_tokens,
                "stream": False
            }
            
            # –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Ollama
            debug_payload = {k: v for k, v in payload.items() if k != "messages"}
            logger.info(f"ü¶ô Payload –¥–ª—è Ollama: {debug_payload}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=payload
            )
            response.raise_for_status()
            ollama_response = response.json()
            
            duration = time.time() - start_time
            
            # –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ç Ollama
            logger.info(f"ü¶ô RAW OLLAMA RESPONSE: {ollama_response}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç Ollama
            ollama_content = ""
            if "choices" in ollama_response and len(ollama_response["choices"]) > 0:
                choice = ollama_response["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    ollama_content = choice["message"]["content"]
            
            logger.info(f"ü¶ô –ö–æ–Ω—Ç–µ–Ω—Ç –æ—Ç Ollama: {ollama_content[:100]}...")
            
            # –ü–û–î–î–ï–õ–´–í–ê–ï–ú Azure OpenAI response! üé≠
            fake_azure_id = f"chatcmpl-{uuid.uuid4().hex[:29]}"
            fake_azure_response = ChatResponse(
                id=fake_azure_id,
                model="gpt-4.1",  # üé≠ –ü–†–ò–¢–í–û–†–Ø–ï–ú–°–Ø —á—Ç–æ —ç—Ç–æ Azure gpt-4.1
                choices=[{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": ollama_content,
                        "tool_calls": []  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º tools
                    },
                    "finish_reason": "stop"
                }],
                usage={
                    "prompt_tokens": 50,  # –§–µ–π–∫–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
                    "completion_tokens": len(ollama_content.split()),
                    "total_tokens": 50 + len(ollama_content.split())
                },
                system_fingerprint="fp_ollama_fake_azure",  # üé≠ –§–µ–π–∫–æ–≤—ã–π fingerprint
                service_tier="default"
            )
            
            logger.info("üé≠ –ü–û–î–î–ï–õ–ê–ù–ù–´–ô Azure response —Å–æ–∑–¥–∞–Ω!", extra={
                "fake_id": fake_azure_id,
                "model": "gpt-4.1",
                "duration_ms": round(duration * 1000, 2),
                "content_length": len(ollama_content)
            })
            
            return fake_azure_response
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error("üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama", extra={
                "error": str(e),
                "error_type": type(e).__name__,
                "duration_ms": round(duration * 1000, 2),
                "session_id": request.session_id
            })
            raise LLMProviderError(f"Ollama error: {str(e)}")
    
    async def create_chat_completion_stream(self, request: ChatRequest) -> AsyncIterator[ChatResponse]:
        """
        Streaming –¥–ª—è Ollama (—Ç–æ–∂–µ –ø—Ä–∏—Ç–≤–æ—Ä—è–µ–º—Å—è Azure)
        """
        start_time = time.time()
        
        logger.info("ü¶ô –ù–∞—á–∏–Ω–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –∫ Ollama (–ø—Ä–∏—Ç–≤–æ—Ä—è–µ–º—Å—è Azure)", extra={
            "model": self.model_name,
            "messages_count": len(request.messages),
            "session_id": request.session_id
        })
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è Ollama streaming
            ollama_messages = [
                {"role": msg.role, "content": msg.content}
                for msg in request.messages
            ]
            
            payload = {
                "model": self.model_name,
                "messages": ollama_messages,
                "temperature": request.temperature or 0.7,
                "max_tokens": request.max_tokens,
                "stream": True
            }
            
            logger.info(f"ü¶ô STREAMING Payload –¥–ª—è Ollama: {payload}")
            
            fake_azure_id = f"chatcmpl-{uuid.uuid4().hex[:29]}"
            chunk_count = 0
            
            # Streaming –∑–∞–ø—Ä–æ—Å –∫ Ollama
            async with self.client.stream(
                "POST",
                f"{self.base_url}/chat/completions",
                json=payload
            ) as response:
                response.raise_for_status()
                
                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        chunk_count += 1
                        if chunk_count == 1:
                            first_chunk_time = time.time() - start_time
                            logger.debug(f"ü¶ô –ü–µ—Ä–≤—ã–π chunk –æ—Ç Ollama –∑–∞ {round(first_chunk_time * 1000, 2)}ms")
                        
                        chunk_data = line[6:]  # –£–±–∏—Ä–∞–µ–º "data: "
                        if chunk_data.strip() == "[DONE]":
                            break
                        
                        try:
                            import json
                            ollama_chunk = json.loads(chunk_data)
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ Ollama chunk
                            content = ""
                            if "choices" in ollama_chunk and len(ollama_chunk["choices"]) > 0:
                                choice = ollama_chunk["choices"][0]
                                if "delta" in choice and "content" in choice["delta"]:
                                    content = choice["delta"]["content"]
                            
                            # –ü–û–î–î–ï–õ–´–í–ê–ï–ú Azure streaming chunk! üé≠
                            fake_chunk = ChatResponse(
                                id=fake_azure_id,
                                model="gpt-4.1",  # üé≠ –ü–†–ò–¢–í–û–†–Ø–ï–ú–°–Ø Azure
                                choices=[{
                                    "index": 0,
                                    "delta": {
                                        "content": content
                                    },
                                    "finish_reason": None
                                }],
                                usage=None
                            )
                            
                            yield fake_chunk
                            
                        except json.JSONDecodeError:
                            continue
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π chunk
            final_chunk = ChatResponse(
                id=fake_azure_id,
                model="gpt-4.1",
                choices=[{
                    "index": 0,
                    "delta": {},
                    "finish_reason": "stop"
                }],
                usage=None
            )
            yield final_chunk
            
            total_duration = time.time() - start_time
            logger.info("ü¶ô Ollama —Å—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω", extra={
                "chunks_received": chunk_count,
                "total_duration_ms": round(total_duration * 1000, 2),
                "session_id": request.session_id
            })
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error("üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–µ Ollama", extra={
                "error": str(e),
                "error_type": type(e).__name__,
                "duration_ms": round(duration * 1000, 2),
                "session_id": request.session_id
            })
            raise LLMProviderError(f"Ollama streaming error: {str(e)}")
    
    async def health_check(self) -> bool:
        logger.debug("ü¶ô –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Ollama...")
        start_time = time.time()
        
        try:
            response = await self.client.get(f"{self.base_url}/models")
            response.raise_for_status()
            
            duration = time.time() - start_time
            logger.info("ü¶ô Ollama health check —É—Å–ø–µ—à–µ–Ω", extra={
                "duration_ms": round(duration * 1000, 2)
            })
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            logger.warning("üî¥ Ollama health check –Ω–µ—É—Å–ø–µ—à–µ–Ω", extra={
                "error": str(e),
                "duration_ms": round(duration * 1000, 2)
            })
            return False 