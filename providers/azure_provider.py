# providers/azure_provider.py - –ü–†–û–°–¢–ê–Ø –í–ï–†–°–ò–Ø –° –ü–û–õ–ù–´–ú –°–¢–†–ò–ú–ò–ù–ì–û–ú

import asyncio
import os
import time
from typing import AsyncIterator
from openai import AsyncAzureOpenAI
from llm_pii_proxy.core.models import ChatRequest, ChatResponse
from llm_pii_proxy.core.interfaces import LLMProvider
from llm_pii_proxy.core.exceptions import LLMProviderError
from llm_pii_proxy.config.settings import settings
from llm_pii_proxy.observability import logger as obs_logger

logger = obs_logger.get_logger(__name__)

class AzureOpenAIProvider(LLMProvider):
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º settings –≤–º–µ—Å—Ç–æ –ø—Ä—è–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.api_key = settings.azure_openai_api_key
        self.endpoint = settings.azure_openai_endpoint
        self.deployment_name = settings.azure_completions_model
        self.api_version = settings.azure_openai_api_version
        
        self.client = AsyncAzureOpenAI(
            api_key=self.api_key,
            azure_endpoint=self.endpoint,
            api_version=self.api_version
        )
        
        logger.debug("‚òÅÔ∏è Azure OpenAI Provider –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω", extra={
            "endpoint": self.endpoint,
            "deployment": self.deployment_name,
            "api_version": self.api_version
        })

    @property
    def provider_name(self) -> str:
        return "azure_openai"

    async def create_chat_completion(self, request: ChatRequest) -> ChatResponse:
        start_time = time.time()
        
        logger.debug("üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Azure OpenAI", extra={
            "model": self.deployment_name,
            "messages_count": len(request.messages),
            "session_id": request.session_id
        })
        
        try:
            # –ü–†–û–ó–†–ê–ß–ù–û–ï –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π - –ë–ï–ó –§–ò–õ–¨–¢–†–ê–¶–ò–ò!
            azure_messages = []
            for msg in request.messages:
                azure_msg = {"role": msg.role, "content": msg.content}
                
                # –î–æ–±–∞–≤–ª—è–µ–º tool_calls –µ—Å–ª–∏ –µ—Å—Ç—å –≤ assistant —Å–æ–æ–±—â–µ–Ω–∏–∏
                if msg.role == "assistant" and hasattr(msg, 'tool_calls') and msg.tool_calls:
                    azure_msg["tool_calls"] = msg.tool_calls
                
                # –î–æ–±–∞–≤–ª—è–µ–º tool_call_id –µ—Å–ª–∏ –µ—Å—Ç—å –≤ tool —Å–æ–æ–±—â–µ–Ω–∏–∏
                if msg.role == "tool" and hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                    azure_msg["tool_call_id"] = msg.tool_call_id
                
                azure_messages.append(azure_msg)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º payload
            payload = {
                "model": self.deployment_name,
                "messages": azure_messages,
                "temperature": request.temperature,
                "max_completion_tokens": request.max_completion_tokens,
                "stream": False
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º tools –µ—Å–ª–∏ –µ—Å—Ç—å
            if request.tools:
                payload["tools"] = request.tools
                logger.debug(f"üîß –î–æ–±–∞–≤–ª–µ–Ω—ã tools: {len(request.tools)}")
            if request.tool_choice:
                payload["tool_choice"] = request.tool_choice
                logger.debug(f"üéØ –î–æ–±–∞–≤–ª–µ–Ω tool_choice: {request.tool_choice}")
            
            logger.debug(f"üì§ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(azure_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –≤ Azure OpenAI")
            
            # üîç –ü–û–õ–ù–ê–Ø –û–¢–õ–ê–î–ö–ê –í–•–û–î–Ø–©–ò–• –°–û–û–ë–©–ï–ù–ò–ô
            logger.debug("üîç –ü–û–õ–ù–´–ô PAYLOAD –î–õ–Ø AZURE OPENAI:")
            import json
            logger.debug(json.dumps(payload, indent=2, ensure_ascii=False))
            
            response = await self.client.chat.completions.create(**payload)
            
            duration = time.time() - start_time
            logger.debug("üì® –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Azure OpenAI", extra={
                "duration_ms": round(duration * 1000, 2),
                "choices": len(response.choices)
            })
            
            # üîç –ü–û–õ–ù–ê–Ø –û–¢–õ–ê–î–ö–ê –û–¢–í–ï–¢–ê –û–¢ AZURE
            logger.debug("üîç –ü–û–õ–ù–´–ô RAW –û–¢–í–ï–¢ –û–¢ AZURE OPENAI:")
            logger.debug(f"   ID: {response.id}")
            logger.debug(f"   Model: {response.model}")
            logger.debug(f"   Object: {response.object}")
            logger.debug(f"   Created: {response.created}")
            logger.debug(f"   Choices count: {len(response.choices)}")
            
            for i, choice in enumerate(response.choices):
                logger.debug(f"   Choice {i}:")
                logger.debug(f"     Index: {choice.index}")
                logger.debug(f"     Finish reason: {choice.finish_reason}")
                logger.debug(f"     Message role: {choice.message.role}")
                logger.debug(f"     Message content: {choice.message.content}")
                
                if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                    logger.debug(f"     Tool calls: {len(choice.message.tool_calls)}")
                    for j, tc in enumerate(choice.message.tool_calls):
                        logger.debug(f"       Tool call {j}: ID={tc.id}, Type={tc.type}")
                        logger.debug(f"       Function: {tc.function.name}")
                        logger.debug(f"       Arguments: {tc.function.arguments}")
                else:
                    logger.debug(f"     Tool calls: None")
            
            if response.usage:
                logger.debug(f"   Usage: prompt={response.usage.prompt_tokens}, completion={response.usage.completion_tokens}, total={response.usage.total_tokens}")
                logger.warning(f"ü™ô –†–∞—Å—Ö–æ–¥ —Ç–æ–∫–µ–Ω–æ–≤ (Azure): prompt={response.usage.prompt_tokens}, completion={response.usage.completion_tokens}, total={response.usage.total_tokens}")
            
            # üîç –ü–û–õ–ù–´–ô JSON DUMP
            import json
            try:
                response_dict = response.model_dump()
                logger.debug("üîç AZURE RESPONSE JSON:")
                logger.debug(json.dumps(response_dict, indent=2, ensure_ascii=False))
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç: {e}")
                logger.debug(f"üîç AZURE RESPONSE STR: {str(response)}")
            
            # –ü–†–û–ó–†–ê–ß–ù–û–ï –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            choices = []
            for choice in response.choices:
                message = {
                    "role": choice.message.role,
                    "content": choice.message.content or ""
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º tool_calls –µ—Å–ª–∏ –µ—Å—Ç—å
                if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                    message["tool_calls"] = [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in choice.message.tool_calls
                    ]
                    logger.debug(f"üîß –û—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç {len(choice.message.tool_calls)} tool_calls")
                
                choices.append({
                    "index": choice.index,
                    "message": message,
                    "finish_reason": choice.finish_reason
                })
            
            final_response = ChatResponse(
                id=response.id,
                model=response.model,
                choices=choices,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                } if response.usage else None
            )
            
            # üîç –ü–û–õ–ù–ê–Ø –û–¢–õ–ê–î–ö–ê –§–ò–ù–ê–õ–¨–ù–û–ì–û –û–¢–í–ï–¢–ê
            logger.debug("üîç –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢ –î–õ–Ø –ö–õ–ò–ï–ù–¢–ê:")
            logger.debug(json.dumps(final_response.model_dump(), indent=2, ensure_ascii=False))
            
            return final_response
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error("üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Azure OpenAI", extra={
                "error": str(e),
                "duration_ms": round(duration * 1000, 2)
            })
            raise LLMProviderError(f"Azure OpenAI error: {str(e)}")

    async def create_chat_completion_stream(self, request: ChatRequest) -> AsyncIterator[ChatResponse]:
        start_time = time.time()
        
        logger.debug("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –∫ Azure OpenAI", extra={
            "model": self.deployment_name,
            "messages_count": len(request.messages),
            "session_id": request.session_id
        })
        
        try:
            # –ü–†–û–ó–†–ê–ß–ù–û–ï –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π - –ë–ï–ó –§–ò–õ–¨–¢–†–ê–¶–ò–ò!
            azure_messages = []
            for msg in request.messages:
                azure_msg = {"role": msg.role, "content": msg.content}
                
                # –î–æ–±–∞–≤–ª—è–µ–º tool_calls –µ—Å–ª–∏ –µ—Å—Ç—å –≤ assistant —Å–æ–æ–±—â–µ–Ω–∏–∏
                if msg.role == "assistant" and hasattr(msg, 'tool_calls') and msg.tool_calls:
                    azure_msg["tool_calls"] = msg.tool_calls
                
                # –î–æ–±–∞–≤–ª—è–µ–º tool_call_id –µ—Å–ª–∏ –µ—Å—Ç—å –≤ tool —Å–æ–æ–±—â–µ–Ω–∏–∏
                if msg.role == "tool" and hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                    azure_msg["tool_call_id"] = msg.tool_call_id
                
                azure_messages.append(azure_msg)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º payload –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
            payload = {
                "model": self.deployment_name,
                "messages": azure_messages,
                #"temperature": request.temperature,
                "max_completion_tokens": request.max_completion_tokens,
                "stream": True
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º tools –µ—Å–ª–∏ –µ—Å—Ç—å
            if request.tools:
                payload["tools"] = request.tools
                logger.debug(f"üîß STREAMING: –î–æ–±–∞–≤–ª–µ–Ω—ã tools: {len(request.tools)}")
            if request.tool_choice:
                payload["tool_choice"] = request.tool_choice
                logger.debug(f"üéØ STREAMING: –î–æ–±–∞–≤–ª–µ–Ω tool_choice: {request.tool_choice}")
            
            logger.debug(f"üîÑ STREAMING: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(azure_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –≤ Azure OpenAI")
            
            # üîç –ü–û–õ–ù–ê–Ø –û–¢–õ–ê–î–ö–ê STREAMING PAYLOAD
            logger.debug("üîç STREAMING PAYLOAD –î–õ–Ø AZURE OPENAI:")
            import json
            logger.debug(json.dumps(payload, indent=2, ensure_ascii=False))
            
            chunk_count = 0
            stream = await self.client.chat.completions.create(**payload)
            
            async for response in stream:
                chunk_count += 1
                if chunk_count == 1:
                    first_chunk_time = time.time() - start_time
                    logger.debug(f"‚ö° –ü–µ—Ä–≤—ã–π chunk –ø–æ–ª—É—á–µ–Ω –∑–∞ {round(first_chunk_time * 1000, 2)}ms")
                
                # üîç –û–¢–õ–ê–î–ö–ê –ö–ê–ñ–î–û–ì–û CHUNK
                logger.debug(f"üîç CHUNK {chunk_count} –û–¢ AZURE:")
                try:
                    chunk_dict = response.model_dump()
                    logger.debug(json.dumps(chunk_dict, indent=2, ensure_ascii=False))
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å chunk: {e}")
                    logger.debug(f"üîç CHUNK {chunk_count} STR: {str(response)}")
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º choices –¥–ª—è streaming response
                choices = []
                for choice in response.choices:
                    delta = {}
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º role —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'role') and choice.delta.role:
                        delta["role"] = choice.delta.role
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º content —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content:
                        delta["content"] = choice.delta.content
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º tool_calls —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:
                        delta["tool_calls"] = []
                        for tc in choice.delta.tool_calls:
                            tool_call = {
                                "index": getattr(tc, 'index', 0),
                                "id": getattr(tc, 'id', None),
                                "type": getattr(tc, 'type', 'function'),
                                "function": {}
                            }
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º function –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                            if hasattr(tc, 'function') and tc.function:
                                if hasattr(tc.function, 'name') and tc.function.name is not None:
                                    tool_call["function"]["name"] = tc.function.name
                                if hasattr(tc.function, 'arguments') and tc.function.arguments is not None:
                                    tool_call["function"]["arguments"] = tc.function.arguments
                            
                            # –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º tool_call –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å –≤ chunk –æ—Ç Azure
                            # Cursor —Å–æ–±–µ—Ä–µ—Ç –≤—Å–µ —á–∞—Å—Ç–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ
                            delta["tool_calls"].append(tool_call)
                    
                    choices.append({
                        "index": choice.index,
                        "delta": delta,
                        "finish_reason": choice.finish_reason
                    })
                
                final_chunk = ChatResponse(
                    id=response.id,
                    model=response.model,
                    choices=choices,
                    usage=None  # Usage –æ–±—ã—á–Ω–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º chunk
                )
                
                # üîç –û–¢–õ–ê–î–ö–ê –§–ò–ù–ê–õ–¨–ù–û–ì–û CHUNK –î–õ–Ø –ö–õ–ò–ï–ù–¢–ê
                logger.debug(f"üîç –§–ò–ù–ê–õ–¨–ù–´–ô CHUNK {chunk_count} –î–õ–Ø –ö–õ–ò–ï–ù–¢–ê:")
                logger.debug(json.dumps(final_chunk.model_dump(), indent=2, ensure_ascii=False))
                
                yield final_chunk
            
            total_duration = time.time() - start_time
            logger.debug("‚úÖ –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω", extra={
                "chunks_received": chunk_count,
                "total_duration_ms": round(total_duration * 1000, 2)
            })
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error("üí• –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–µ Azure OpenAI", extra={
                "error": str(e),
                "duration_ms": round(duration * 1000, 2)
            })
            raise LLMProviderError(f"Azure OpenAI streaming error: {str(e)}")

    async def health_check(self) -> bool:
        logger.debug("üè• –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è Azure OpenAI...")
        
        try:
            response = await self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[{"role": "user", "content": "ping"}],
                max_completion_tokens=1
            )
            logger.debug("üíö Azure OpenAI health check —É—Å–ø–µ—à–µ–Ω")
            return True
            
        except Exception as e:
            logger.warning(f"üî¥ Azure OpenAI health check –Ω–µ—É—Å–ø–µ—à–µ–Ω: {str(e)}")
            return False

# -----------------------------------------------------------------------------
# Optional: dynamically replace Azure provider with OpenRouterProvider
# This enables using OpenRouter without touching chat.py or llm_service.py.
# -----------------------------------------------------------------------------
import os as _os
if _os.getenv("USE_OPENROUTER", "false").lower() == "true":
    from .openrouter_provider import OpenRouterProvider as AzureOpenAIProvider  # type: ignore 