# providers/azure_provider.py - ĞŸĞ ĞĞ¡Ğ¢ĞĞ¯ Ğ’Ğ•Ğ Ğ¡Ğ˜Ğ¯ Ğ¡ ĞŸĞĞ›ĞĞ«Ğœ Ğ¡Ğ¢Ğ Ğ˜ĞœĞ˜ĞĞ“ĞĞœ

import asyncio
import os
import time
from typing import AsyncIterator
from openai import AsyncAzureOpenAI
from llm_pii_proxy.core.models import ChatRequest, ChatResponse
from llm_pii_proxy.core.interfaces import LLMProvider
from llm_pii_proxy.core.exceptions import LLMProviderError
from llm_pii_proxy.config.settings import settings
from llm_pii_proxy.observability import logger as obs_logger

logger = obs_logger.get_logger(__name__)

class AzureOpenAIProvider(LLMProvider):
    def __init__(self):
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ settings Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ñ€ÑĞ¼Ñ‹Ñ… Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
        self.api_key = settings.azure_openai_api_key
        self.endpoint = settings.azure_openai_endpoint
        self.deployment_name = settings.azure_completions_model
        self.api_version = settings.azure_openai_api_version
        
        self.client = AsyncAzureOpenAI(
            api_key=self.api_key,
            azure_endpoint=self.endpoint,
            api_version=self.api_version
        )
        
        logger.debug("â˜ï¸ Azure OpenAI Provider Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½", extra={
            "endpoint": self.endpoint,
            "deployment": self.deployment_name,
            "api_version": self.api_version
        })

    @property
    def provider_name(self) -> str:
        return "azure_openai"

    async def create_chat_completion(self, request: ChatRequest) -> ChatResponse:
        start_time = time.time()
        
        logger.debug("ğŸ“¤ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğº Azure OpenAI", extra={
            "model": self.deployment_name,
            "messages_count": len(request.messages),
            "session_id": request.session_id
        })
        
        try:
            # ĞŸĞ ĞĞ—Ğ ĞĞ§ĞĞĞ• Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ - Ğ‘Ğ•Ğ— Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ ĞĞ¦Ğ˜Ğ˜!
            azure_messages = []
            for msg in request.messages:
                azure_msg = {"role": msg.role, "content": msg.content}
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tool_calls ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ² assistant ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸
                if msg.role == "assistant" and hasattr(msg, 'tool_calls') and msg.tool_calls:
                    azure_msg["tool_calls"] = msg.tool_calls
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tool_call_id ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ² tool ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸
                if msg.role == "tool" and hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                    azure_msg["tool_call_id"] = msg.tool_call_id
                
                azure_messages.append(azure_msg)
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ payload
            payload = {
                "model": self.deployment_name,
                "messages": azure_messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "stream": False
            }
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tools ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if request.tools:
                payload["tools"] = request.tools
                logger.debug(f"ğŸ”§ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ tools: {len(request.tools)}")
            if request.tool_choice:
                payload["tool_choice"] = request.tool_choice
                logger.debug(f"ğŸ¯ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ tool_choice: {request.tool_choice}")
            
            logger.debug(f"ğŸ“¤ ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ {len(azure_messages)} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Azure OpenAI")
            
            # ğŸ” ĞŸĞĞ›ĞĞĞ¯ ĞĞ¢Ğ›ĞĞ”ĞšĞ Ğ’Ğ¥ĞĞ”Ğ¯Ğ©Ğ˜Ğ¥ Ğ¡ĞĞĞ‘Ğ©Ğ•ĞĞ˜Ğ™
            logger.debug("ğŸ” ĞŸĞĞ›ĞĞ«Ğ™ PAYLOAD Ğ”Ğ›Ğ¯ AZURE OPENAI:")
            import json
            logger.debug(json.dumps(payload, indent=2, ensure_ascii=False))
            
            response = await self.client.chat.completions.create(**payload)
            
            duration = time.time() - start_time
            logger.debug("ğŸ“¨ ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ¾Ñ‚ Azure OpenAI", extra={
                "duration_ms": round(duration * 1000, 2),
                "choices": len(response.choices)
            })
            
            # ğŸ” ĞŸĞĞ›ĞĞĞ¯ ĞĞ¢Ğ›ĞĞ”ĞšĞ ĞĞ¢Ğ’Ğ•Ğ¢Ğ ĞĞ¢ AZURE
            logger.debug("ğŸ” ĞŸĞĞ›ĞĞ«Ğ™ RAW ĞĞ¢Ğ’Ğ•Ğ¢ ĞĞ¢ AZURE OPENAI:")
            logger.debug(f"   ID: {response.id}")
            logger.debug(f"   Model: {response.model}")
            logger.debug(f"   Object: {response.object}")
            logger.debug(f"   Created: {response.created}")
            logger.debug(f"   Choices count: {len(response.choices)}")
            
            for i, choice in enumerate(response.choices):
                logger.debug(f"   Choice {i}:")
                logger.debug(f"     Index: {choice.index}")
                logger.debug(f"     Finish reason: {choice.finish_reason}")
                logger.debug(f"     Message role: {choice.message.role}")
                logger.debug(f"     Message content: {choice.message.content}")
                
                if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                    logger.debug(f"     Tool calls: {len(choice.message.tool_calls)}")
                    for j, tc in enumerate(choice.message.tool_calls):
                        logger.debug(f"       Tool call {j}: ID={tc.id}, Type={tc.type}")
                        logger.debug(f"       Function: {tc.function.name}")
                        logger.debug(f"       Arguments: {tc.function.arguments}")
                else:
                    logger.debug(f"     Tool calls: None")
            
            if response.usage:
                logger.debug(f"   Usage: prompt={response.usage.prompt_tokens}, completion={response.usage.completion_tokens}, total={response.usage.total_tokens}")
            
            # ğŸ” ĞŸĞĞ›ĞĞ«Ğ™ JSON DUMP
            import json
            try:
                response_dict = response.model_dump()
                logger.debug("ğŸ” AZURE RESPONSE JSON:")
                logger.debug(json.dumps(response_dict, indent=2, ensure_ascii=False))
            except Exception as e:
                logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‚: {e}")
                logger.debug(f"ğŸ” AZURE RESPONSE STR: {str(response)}")
            
            # ĞŸĞ ĞĞ—Ğ ĞĞ§ĞĞĞ• Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
            choices = []
            for choice in response.choices:
                message = {
                    "role": choice.message.role,
                    "content": choice.message.content or ""
                }
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tool_calls ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
                if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                    message["tool_calls"] = [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in choice.message.tool_calls
                    ]
                    logger.debug(f"ğŸ”§ ĞÑ‚Ğ²ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ {len(choice.message.tool_calls)} tool_calls")
                
                choices.append({
                    "index": choice.index,
                    "message": message,
                    "finish_reason": choice.finish_reason
                })
            
            final_response = ChatResponse(
                id=response.id,
                model=response.model,
                choices=choices,
                usage={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                } if response.usage else None
            )
            
            # ğŸ” ĞŸĞĞ›ĞĞĞ¯ ĞĞ¢Ğ›ĞĞ”ĞšĞ Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ“Ğ ĞĞ¢Ğ’Ğ•Ğ¢Ğ
            logger.debug("ğŸ” Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ™ ĞĞ¢Ğ’Ğ•Ğ¢ Ğ”Ğ›Ğ¯ ĞšĞ›Ğ˜Ğ•ĞĞ¢Ğ:")
            logger.debug(json.dumps(final_response.model_dump(), indent=2, ensure_ascii=False))
            
            return final_response
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error("ğŸ’¥ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ Ğº Azure OpenAI", extra={
                "error": str(e),
                "duration_ms": round(duration * 1000, 2)
            })
            raise LLMProviderError(f"Azure OpenAI error: {str(e)}")

    async def create_chat_completion_stream(self, request: ChatRequest) -> AsyncIterator[ChatResponse]:
        start_time = time.time()
        
        logger.debug("ğŸ”„ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğº Azure OpenAI", extra={
            "model": self.deployment_name,
            "messages_count": len(request.messages),
            "session_id": request.session_id
        })
        
        try:
            # ĞŸĞ ĞĞ—Ğ ĞĞ§ĞĞĞ• Ğ¿Ñ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ - Ğ‘Ğ•Ğ— Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ ĞĞ¦Ğ˜Ğ˜!
            azure_messages = []
            for msg in request.messages:
                azure_msg = {"role": msg.role, "content": msg.content}
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tool_calls ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ² assistant ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸
                if msg.role == "assistant" and hasattr(msg, 'tool_calls') and msg.tool_calls:
                    azure_msg["tool_calls"] = msg.tool_calls
                
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tool_call_id ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ğ² tool ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¸
                if msg.role == "tool" and hasattr(msg, 'tool_call_id') and msg.tool_call_id:
                    azure_msg["tool_call_id"] = msg.tool_call_id
                
                azure_messages.append(azure_msg)
            
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ payload Ğ´Ğ»Ñ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğ°
            payload = {
                "model": self.deployment_name,
                "messages": azure_messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "stream": True
            }
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tools ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ
            if request.tools:
                payload["tools"] = request.tools
                logger.debug(f"ğŸ”§ STREAMING: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ tools: {len(request.tools)}")
            if request.tool_choice:
                payload["tool_choice"] = request.tool_choice
                logger.debug(f"ğŸ¯ STREAMING: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ tool_choice: {request.tool_choice}")
            
            logger.debug(f"ğŸ”„ STREAMING: ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ {len(azure_messages)} ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² Azure OpenAI")
            
            # ğŸ” ĞŸĞĞ›ĞĞĞ¯ ĞĞ¢Ğ›ĞĞ”ĞšĞ STREAMING PAYLOAD
            logger.debug("ğŸ” STREAMING PAYLOAD Ğ”Ğ›Ğ¯ AZURE OPENAI:")
            import json
            logger.debug(json.dumps(payload, indent=2, ensure_ascii=False))
            
            chunk_count = 0
            stream = await self.client.chat.completions.create(**payload)
            
            async for response in stream:
                chunk_count += 1
                if chunk_count == 1:
                    first_chunk_time = time.time() - start_time
                    logger.debug(f"âš¡ ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ chunk Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½ Ğ·Ğ° {round(first_chunk_time * 1000, 2)}ms")
                
                # ğŸ” ĞĞ¢Ğ›ĞĞ”ĞšĞ ĞšĞĞ–Ğ”ĞĞ“Ğ CHUNK
                logger.debug(f"ğŸ” CHUNK {chunk_count} ĞĞ¢ AZURE:")
                try:
                    chunk_dict = response.model_dump()
                    logger.debug(json.dumps(chunk_dict, indent=2, ensure_ascii=False))
                except Exception as e:
                    logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ chunk: {e}")
                    logger.debug(f"ğŸ” CHUNK {chunk_count} STR: {str(response)}")
                
                # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ choices Ğ´Ğ»Ñ streaming response
                choices = []
                for choice in response.choices:
                    delta = {}
                    
                    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ role Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑÑ‚ÑŒ
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'role') and choice.delta.role:
                        delta["role"] = choice.delta.role
                    
                    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ content Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑÑ‚ÑŒ
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content:
                        delta["content"] = choice.delta.content
                    
                    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tool_calls Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ¸ ĞµÑÑ‚ÑŒ
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'tool_calls') and choice.delta.tool_calls:
                        delta["tool_calls"] = []
                        for tc in choice.delta.tool_calls:
                            tool_call = {
                                "index": getattr(tc, 'index', 0),
                                "id": getattr(tc, 'id', None),
                                "type": getattr(tc, 'type', 'function'),
                                "function": {}
                            }
                            
                            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ function Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞµÑĞ»Ğ¸ Ğ¾Ğ½Ğ¸ ĞµÑÑ‚ÑŒ
                            if hasattr(tc, 'function') and tc.function:
                                if hasattr(tc.function, 'name') and tc.function.name is not None:
                                    tool_call["function"]["name"] = tc.function.name
                                if hasattr(tc.function, 'arguments') and tc.function.arguments is not None:
                                    tool_call["function"]["arguments"] = tc.function.arguments
                            
                            # Ğ’Ğ¡Ğ•Ğ“Ğ”Ğ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ tool_call ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑÑ‚ÑŒ Ğ² chunk Ğ¾Ñ‚ Azure
                            # Cursor ÑĞ¾Ğ±ĞµÑ€ĞµÑ‚ Ğ²ÑĞµ Ñ‡Ğ°ÑÑ‚Ğ¸ ÑĞ°Ğ¼Ğ¾ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾
                            delta["tool_calls"].append(tool_call)
                    
                    choices.append({
                        "index": choice.index,
                        "delta": delta,
                        "finish_reason": choice.finish_reason
                    })
                
                final_chunk = ChatResponse(
                    id=response.id,
                    model=response.model,
                    choices=choices,
                    usage=None  # Usage Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¼ chunk
                )
                
                # ğŸ” ĞĞ¢Ğ›ĞĞ”ĞšĞ Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞĞ“Ğ CHUNK Ğ”Ğ›Ğ¯ ĞšĞ›Ğ˜Ğ•ĞĞ¢Ğ
                logger.debug(f"ğŸ” Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ™ CHUNK {chunk_count} Ğ”Ğ›Ğ¯ ĞšĞ›Ğ˜Ğ•ĞĞ¢Ğ:")
                logger.debug(json.dumps(final_chunk.model_dump(), indent=2, ensure_ascii=False))
                
                yield final_chunk
            
            total_duration = time.time() - start_time
            logger.debug("âœ… Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½", extra={
                "chunks_received": chunk_count,
                "total_duration_ms": round(total_duration * 1000, 2)
            })
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error("ğŸ’¥ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ ÑÑ‚Ñ€Ğ¸Ğ¼Ğ¸Ğ½Ğ³Ğµ Azure OpenAI", extra={
                "error": str(e),
                "duration_ms": round(duration * 1000, 2)
            })
            raise LLMProviderError(f"Azure OpenAI streaming error: {str(e)}")

    async def health_check(self) -> bool:
        logger.debug("ğŸ¥ ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ´Ğ¾Ñ€Ğ¾Ğ²ÑŒÑ Azure OpenAI...")
        
        try:
            response = await self.client.chat.completions.create(
                model=self.deployment_name,
                messages=[{"role": "user", "content": "ping"}],
                max_tokens=1
            )
            logger.debug("ğŸ’š Azure OpenAI health check ÑƒÑĞ¿ĞµÑˆĞµĞ½")
            return True
            
        except Exception as e:
            logger.warning(f"ğŸ”´ Azure OpenAI health check Ğ½ĞµÑƒÑĞ¿ĞµÑˆĞµĞ½: {str(e)}")
            return False 