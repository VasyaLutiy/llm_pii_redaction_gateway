# api/routes/chat.py

import time
import json
import asyncio
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
from llm_pii_proxy.core.models import ChatRequest, ChatResponse
from llm_pii_proxy.core.exceptions import PIIProcessingError, LLMProviderError, ConfigurationError, ValidationError
from llm_pii_proxy.services.llm_service import LLMService
from llm_pii_proxy.providers.azure_provider import AzureOpenAIProvider
from llm_pii_proxy.security.pii_gateway import AsyncPIISecurityGateway
from llm_pii_proxy.cache.context_deduplication import get_context_cache
from llm_pii_proxy.observability import logger as obs_logger

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä
logger = obs_logger.get_logger(__name__)

router = APIRouter()

# For now, instantiate dependencies here (in production, use DI)
# üïµÔ∏è‚Äç‚ôÇÔ∏è –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É Azure –∏ Ollama
import os
USE_OLLAMA = os.getenv("USE_OLLAMA", "false").lower() == "true"

if USE_OLLAMA:
    from llm_pii_proxy.providers.ollama_provider import OllamaProvider
    llm_provider = OllamaProvider()
    logger.debug("ü¶ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama provider (–ø—Ä–∏—Ç–≤–æ—Ä—è–µ—Ç—Å—è Azure)")
else:
    llm_provider = AzureOpenAIProvider()
    logger.debug("‚òÅÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º Azure OpenAI provider")

pii_gateway = AsyncPIISecurityGateway()
llm_service = LLMService(llm_provider, pii_gateway)

MAX_MESSAGES = 50  # –õ–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è LLM

def validate_chat_request(request: ChatRequest) -> None:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥—è—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    if not request.messages:
        raise ValidationError("Messages cannot be empty")
    
    if len(request.messages) > 1000:  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç
        raise ValidationError("Too many messages (max 1000)")
    
    for i, message in enumerate(request.messages):
        # –†–∞–∑—Ä–µ—à–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è tool calls –∏ assistant —Å–æ–æ–±—â–µ–Ω–∏–π
        if message.role not in ["tool", "assistant"] and (not message.content or not message.content.strip()):
            raise ValidationError(f"Message {i+1} content cannot be empty")
        
        if message.content and len(message.content) > 50000:  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏—è
            raise ValidationError(f"Message {i+1} is too long (max 50000 characters)")
    
    if request.temperature is not None and (request.temperature < 0 or request.temperature > 2):
        raise ValidationError("Temperature must be between 0 and 2")
    
    if request.max_completion_tokens is not None and (request.max_completion_tokens < 1 or request.max_completion_tokens > 4000):
        raise ValidationError("Max tokens must be between 1 and 4000")

@router.post("/v1/chat/completions", response_model=ChatResponse)
async def chat_completions(request: ChatRequest, request_body: Request):
    start_time = time.time()
    
    # –û–±—Ä–µ–∑–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –¥–æ MAX_MESSAGES
    original_count = len(request.messages)
    if original_count > MAX_MESSAGES:
        request.messages = request.messages[-MAX_MESSAGES:]
        logger.info(f"[Cursor] –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –æ–±—Ä–µ–∑–∞–Ω–∞ —Å {original_count} –¥–æ {MAX_MESSAGES}")
    
    # –ü–æ–ª—É—á–∞–µ–º raw body –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    body = await request_body.body()
    raw_request = body.decode('utf-8')
    
    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    headers = dict(request_body.headers)
    
    logger.debug("üåü –ü–æ–ª—É—á–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ chat completions", extra={
        "endpoint": "/v1/chat/completions",
        "session_id": request.session_id,
        "model": request.model,
        "messages_count": len(request.messages),
        "total_input_length": sum(len(msg.content) for msg in request.messages),
        "has_tools": request.tools is not None,
        "has_tool_choice": request.tool_choice is not None,
        "has_functions": request.functions is not None,
        "stream": getattr(request, 'stream', False),
        "user_agent": headers.get("user-agent", "unknown"),
        "client_ip": headers.get("x-forwarded-for", "unknown")
    })
    
    # –õ–æ–≥–∏—Ä—É–µ–º raw request –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    logger.debug(f"üì• RAW REQUEST FROM CLIENT: {raw_request}")
    logger.debug(f"üì• REQUEST HEADERS: {headers}")
    
    # üîç –ü–û–õ–ù–ê–Ø –û–¢–õ–ê–î–ö–ê –í–•–û–î–Ø–©–ï–ì–û –ó–ê–ü–†–û–°–ê
    logger.debug("üîç –ü–û–õ–ù–´–ô PARSED REQUEST:")
    logger.debug(json.dumps(request.model_dump(), indent=2, ensure_ascii=False))
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å tools, –ª–æ–≥–∏—Ä—É–µ–º –∏—Ö
    if request.tools:
        logger.debug(f"üîß TOOLS –≤ –∑–∞–ø—Ä–æ—Å–µ: {len(request.tools)} tools")
        for i, tool in enumerate(request.tools):
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤: Anthropic (name) –∏ OpenAI (function.name)
            tool_name = tool.get('name') or tool.get('function', {}).get('name', 'unknown')
            logger.debug(f"    Tool {i+1}: {tool_name}")
    
    if request.tool_choice:
        logger.debug(f"üéØ TOOL_CHOICE –≤ –∑–∞–ø—Ä–æ—Å–µ: {request.tool_choice}")
    
    if request.functions:
        logger.debug(f"‚öôÔ∏è FUNCTIONS –≤ –∑–∞–ø—Ä–æ—Å–µ: {len(request.functions)} functions")
    
    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π
    logger.debug("üìù –î–ï–¢–ê–õ–¨–ù–´–ï –°–û–û–ë–©–ï–ù–ò–Ø:")
    for i, msg in enumerate(request.messages):
        logger.debug(f"    –°–æ–æ–±—â–µ–Ω–∏–µ {i+1}:")
        logger.debug(f"      Role: {msg.role}")
        logger.debug(f"      Content: {msg.content[:200]}{'...' if len(msg.content) > 200 else ''}")
        if hasattr(msg, 'tool_calls') and msg.tool_calls:
            logger.debug(f"      Tool calls: {len(msg.tool_calls)}")
        if hasattr(msg, 'tool_call_id') and msg.tool_call_id:
            logger.debug(f"      Tool call ID: {msg.tool_call_id}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Cursor
    logger.info(f"[Cursor] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∑–∞–ø—Ä–æ—Å–µ: {len(request.messages)}")
    
    # üéØ –¢–†–û–õ–õ–¨-–†–ï–ñ–ò–ú: –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –í–°–ï —Å–æ–æ–±—â–µ–Ω–∏—è Cursor
    total_content_length = sum(len(msg.content or "") for msg in request.messages)
    logger.info(f"üéØ –ü–û–õ–ù–´–ô –†–ê–ó–ú–ï–† –í–°–ï–• –°–û–û–ë–©–ï–ù–ò–ô: {total_content_length} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å—å –∑–∞–ø—Ä–æ—Å –≤ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    try:
        full_request = {
            "model": request.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in request.messages],
            "tools": request.tools if request.tools else "No tools",
            "total_length": total_content_length
        }
        
        with open("cursor_full_request.json", "w", encoding="utf-8") as f:
            json.dump(full_request, f, ensure_ascii=False, indent=2)
        
        logger.info(f"üíæ –ü–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ cursor_full_request.json")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
    if total_content_length > 10000:  # –ï—Å–ª–∏ –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä –±–æ–ª—å—à–µ 10k —Å–∏–º–≤–æ–ª–æ–≤
        logger.info("üî• –í–ö–õ–Æ–ß–ê–ï–ú –ê–ì–†–ï–°–°–ò–í–ù–£–Æ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Æ CURSOR PAYLOAD!")
        
        optimized_messages = []
        
        for i, msg in enumerate(request.messages):
            if msg.role == "system" and len(msg.content or "") > 3000:
                # –ó–∞–º–µ–Ω—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–π
                try:
                    with open("cursor_system_prompt_optimized.txt", "r", encoding="utf-8") as f:
                        optimized_prompt = f.read()
                    optimized_messages.append(type(msg)(role=msg.role, content=optimized_prompt))
                    logger.info(f"‚úÇÔ∏è –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å–æ–∫—Ä–∞—â–µ–Ω —Å {len(msg.content)} –¥–æ {len(optimized_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
                except FileNotFoundError:
                    # Fallback
                    short = msg.content[:1000] + "\n[...–£–†–ï–ó–ê–ù–û...]\n" + msg.content[-500:]
                    optimized_messages.append(type(msg)(role=msg.role, content=short))
                    logger.info(f"‚úÇÔ∏è –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —É—Ä–µ–∑–∞–Ω –¥–æ {len(short)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
            elif msg.role == "user" and "<project_layout>" in (msg.content or ""):
                # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –∑–∞–º–µ–Ω—è–µ–º project_layout –Ω–∞ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                lines = msg.content.split('\n')
                short_content = []
                in_project_layout = False
                
                for line in lines:
                    if "<project_layout>" in line:
                        in_project_layout = True
                        short_content.append(line)
                        short_content.append("Project structure available via list_dir, file_search, and other tools.")
                        short_content.append("Use tools to explore the codebase as needed.")
                    elif "</project_layout>" in line:
                        in_project_layout = False
                        short_content.append(line)
                    elif not in_project_layout:
                        short_content.append(line)
                
                new_content = '\n'.join(short_content)
                optimized_messages.append(type(msg)(role=msg.role, content=new_content))
                logger.info(f"üóÇÔ∏è Project layout –£–†–ï–ó–ê–ù —Å {len(msg.content)} –¥–æ {len(new_content)} —Å–∏–º–≤–æ–ª–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—è {len(msg.content) - len(new_content)})")
                
            elif msg.role == "user" and len(msg.content or "") > 5000:
                # –õ—é–±–æ–µ –¥—Ä—É–≥–æ–µ –¥–ª–∏–Ω–Ω–æ–µ user —Å–æ–æ–±—â–µ–Ω–∏–µ —É—Ä–µ–∑–∞–µ–º
                short = msg.content[:2000] + "\n[...–°–û–î–ï–†–ñ–ò–ú–û–ï –£–†–ï–ó–ê–ù–û –î–õ–Ø –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò...]\n" + msg.content[-1000:]
                optimized_messages.append(type(msg)(role=msg.role, content=short))
                logger.info(f"üìù –î–ª–∏–Ω–Ω–æ–µ user —Å–æ–æ–±—â–µ–Ω–∏–µ —É—Ä–µ–∑–∞–Ω–æ —Å {len(msg.content)} –¥–æ {len(short)} —Å–∏–º–≤–æ–ª–æ–≤")
                
            else:
                # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                optimized_messages.append(msg)
        
        # –ó–∞–º–µ–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        request.messages = optimized_messages
        
        new_total = sum(len(msg.content or "") for msg in request.messages)
        logger.info(f"üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê: {total_content_length} ‚Üí {new_total} —Å–∏–º–≤–æ–ª–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—è {total_content_length - new_total})")
    
    # –ù–ï —Ç—Ä–æ–≥–∞–µ–º tools - –æ–Ω–∏ –≤–∞–∂–Ω—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ Cursor
    if request.tools:
        logger.info(f"üîß TOOLS —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {len(request.tools)} –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤")
    
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        validate_chat_request(request)
        
        # üß† CONTEXT CACHING: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        context_cache = get_context_cache()
        cached_request, deduplication_map = context_cache.deduplicate_request(request)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ LLM
        if deduplication_map:
            logger.info(f"üß† Context cache –ø—Ä–∏–º–µ–Ω–µ–Ω: {len(deduplication_map)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
        
        if getattr(cached_request, 'stream', False):
            async def event_generator():
                async for chunk in llm_service.process_chat_request_stream(cached_request):
                    # –§–æ—Ä–º–∏—Ä—É–µ–º OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π stream-—á–∞–Ω–∫
                    choices = []
                    for choice in chunk.choices:
                        delta = {}
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø–æ–ª—è –∏–∑ delta, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                        if "delta" in choice:
                            if "role" in choice["delta"] and choice["delta"]["role"]:
                                delta["role"] = choice["delta"]["role"]
                            if "content" in choice["delta"] and choice["delta"]["content"] is not None:
                                # üîÑ –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –ø–æ—Ç–æ–∫–æ–≤–æ–º –æ—Ç–≤–µ—Ç–µ
                                content = choice["delta"]["content"]
                                if deduplication_map:
                                    content = context_cache.restore_content(content, deduplication_map)
                                delta["content"] = content
                            if "tool_calls" in choice["delta"] and choice["delta"]["tool_calls"]:
                                delta["tool_calls"] = choice["delta"]["tool_calls"]
                        
                        choices.append({
                            "delta": delta,
                            "index": choice["index"],
                            "finish_reason": choice.get("finish_reason")
                        })
                    
                    stream_chunk = {
                        "id": chunk.id,
                        "object": "chat.completion.chunk",
                        "created": chunk.created,
                        "model": chunk.model,
                        "choices": choices
                    }
                    
                    yield f"data: {json.dumps(stream_chunk, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0)  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ [DONE]
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(event_generator(), media_type="text/event-stream")
        
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        response = await llm_service.process_chat_request(cached_request)
        
        # üîÑ CONTEXT CACHE RESTORE: –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –æ—Ç–≤–µ—Ç–µ
        if deduplication_map and response.choices:
            for choice in response.choices:
                if hasattr(choice, 'message') and hasattr(choice['message'], 'content'):
                    choice['message']['content'] = context_cache.restore_content(
                        choice['message']['content'], deduplication_map
                    )
                elif isinstance(choice, dict) and 'message' in choice and 'content' in choice['message']:
                    choice['message']['content'] = context_cache.restore_content(
                        choice['message']['content'], deduplication_map
                    )
        
        duration = time.time() - start_time
        logger.debug("‚ú® –ó–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω", extra={
            "session_id": request.session_id,
            "total_duration_ms": round(duration * 1000, 2),
            "response_choices": len(response.choices),
            "response_id": response.id
        })
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        logger.debug("üì§ RESPONSE TO CLIENT: %s", response.json())
        print("üì§ RESPONSE TO CLIENT:", response.json())
        
        return response
        
    except ValidationError as e:
        logger.warning(f"‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except PIIProcessingError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PII: {str(e)}")
        raise HTTPException(status_code=500, detail="PII processing error")
    except LLMProviderError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {str(e)}")
        raise HTTPException(status_code=502, detail="LLM provider error")
    except ConfigurationError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {str(e)}")
        raise HTTPException(status_code=500, detail="Configuration error")
    except Exception as e:
        duration = time.time() - start_time
        logger.error("üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞", extra={
            "session_id": request.session_id,
            "error": str(e),
            "error_type": type(e).__name__,
            "duration_ms": round(duration * 1000, 2)
        })
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/v1/models")
async def list_models():
    """OpenAI-compatible endpoint for model listing."""
    return {
        "object": "list",
        "data": [
            {
                "id": "gpt-4.1",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "azure-openai",
                "permission": [],
                "capabilities": {
                    "tools": True,
                    "tool_choice": True,
                    "functions": True,
                    "function_calling": True,
                    "streaming": True
                }
            },
            {
                "id": "pii_llm_v1",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "pii-proxy",
                "permission": [],
                "capabilities": {
                    "tools": True,
                    "tool_choice": True,
                    "functions": True,
                    "function_calling": True,
                    "streaming": True
                }
            }
        ]
    } 