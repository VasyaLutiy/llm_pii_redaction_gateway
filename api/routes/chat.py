# api/routes/chat.py

import time
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse
import json
import asyncio
from llm_pii_proxy.core.models import ChatRequest, ChatResponse
from llm_pii_proxy.core.exceptions import PIIProcessingError, LLMProviderError, ConfigurationError, ValidationError
from llm_pii_proxy.services.llm_service import LLMService
from llm_pii_proxy.providers.azure_provider import AzureOpenAIProvider
from llm_pii_proxy.security.pii_gateway import AsyncPIISecurityGateway
from llm_pii_proxy.observability import logger as obs_logger

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä
logger = obs_logger.get_logger(__name__)

router = APIRouter()

# For now, instantiate dependencies here (in production, use DI)
# üïµÔ∏è‚Äç‚ôÇÔ∏è –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É Azure –∏ Ollama
import os
USE_OLLAMA = os.getenv("USE_OLLAMA", "false").lower() == "true"

if USE_OLLAMA:
    from llm_pii_proxy.providers.ollama_provider import OllamaProvider
    llm_provider = OllamaProvider()
    logger.debug("ü¶ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢: –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama provider (–ø—Ä–∏—Ç–≤–æ—Ä—è–µ—Ç—Å—è Azure)")
else:
    llm_provider = AzureOpenAIProvider()
    logger.debug("‚òÅÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º Azure OpenAI provider")

pii_gateway = AsyncPIISecurityGateway()
llm_service = LLMService(llm_provider, pii_gateway)

MAX_MESSAGES = 50  # –õ–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è LLM

def validate_chat_request(request: ChatRequest) -> None:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥—è—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    if not request.messages:
        raise ValidationError("Messages cannot be empty")
    
    if len(request.messages) > 1000:  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç
        raise ValidationError("Too many messages (max 1000)")
    
    for i, message in enumerate(request.messages):
        # –†–∞–∑—Ä–µ—à–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è tool calls –∏ assistant —Å–æ–æ–±—â–µ–Ω–∏–π
        if message.role not in ["tool", "assistant"] and (not message.content or not message.content.strip()):
            raise ValidationError(f"Message {i+1} content cannot be empty")
        
        if message.content and len(message.content) > 50000:  # –†–∞–∑—É–º–Ω—ã–π –ª–∏–º–∏—Ç –Ω–∞ —Ä–∞–∑–º–µ—Ä —Å–æ–æ–±—â–µ–Ω–∏—è
            raise ValidationError(f"Message {i+1} is too long (max 50000 characters)")
    
    if request.temperature is not None and (request.temperature < 0 or request.temperature > 2):
        raise ValidationError("Temperature must be between 0 and 2")
    
    if request.max_tokens is not None and (request.max_tokens < 1 or request.max_tokens > 4000):
        raise ValidationError("Max tokens must be between 1 and 4000")

@router.post("/v1/chat/completions", response_model=ChatResponse)
async def chat_completions(request: ChatRequest, request_body: Request):
    start_time = time.time()
    
    # –û–±—Ä–µ–∑–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –¥–æ MAX_MESSAGES
    original_count = len(request.messages)
    if original_count > MAX_MESSAGES:
        request.messages = request.messages[-MAX_MESSAGES:]
        logger.info(f"[Cursor] –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –æ–±—Ä–µ–∑–∞–Ω–∞ —Å {original_count} –¥–æ {MAX_MESSAGES}")
    
    # –ü–æ–ª—É—á–∞–µ–º raw body –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    body = await request_body.body()
    raw_request = body.decode('utf-8')
    
    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    headers = dict(request_body.headers)
    
    logger.debug("üåü –ü–æ–ª—É—á–µ–Ω –Ω–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ chat completions", extra={
        "endpoint": "/v1/chat/completions",
        "session_id": request.session_id,
        "model": request.model,
        "messages_count": len(request.messages),
        "total_input_length": sum(len(msg.content) for msg in request.messages),
        "has_tools": request.tools is not None,
        "has_tool_choice": request.tool_choice is not None,
        "has_functions": request.functions is not None,
        "stream": getattr(request, 'stream', False),
        "user_agent": headers.get("user-agent", "unknown"),
        "client_ip": headers.get("x-forwarded-for", "unknown")
    })
    
    # –õ–æ–≥–∏—Ä—É–µ–º raw request –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    logger.debug(f"üì• RAW REQUEST FROM CLIENT: {raw_request}")
    logger.debug(f"üì• REQUEST HEADERS: {headers}")
    
    # üîç –ü–û–õ–ù–ê–Ø –û–¢–õ–ê–î–ö–ê –í–•–û–î–Ø–©–ï–ì–û –ó–ê–ü–†–û–°–ê
    logger.debug("üîç –ü–û–õ–ù–´–ô PARSED REQUEST:")
    logger.debug(json.dumps(request.model_dump(), indent=2, ensure_ascii=False))
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å tools, –ª–æ–≥–∏—Ä—É–µ–º –∏—Ö
    if request.tools:
        logger.debug(f"üîß TOOLS –≤ –∑–∞–ø—Ä–æ—Å–µ: {len(request.tools)} tools")
        for i, tool in enumerate(request.tools):
            logger.debug(f"    Tool {i+1}: {tool.get('function', {}).get('name', 'unknown')}")
    
    if request.tool_choice:
        logger.debug(f"üéØ TOOL_CHOICE –≤ –∑–∞–ø—Ä–æ—Å–µ: {request.tool_choice}")
    
    if request.functions:
        logger.debug(f"‚öôÔ∏è FUNCTIONS –≤ –∑–∞–ø—Ä–æ—Å–µ: {len(request.functions)} functions")
    
    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π
    logger.debug("üìù –î–ï–¢–ê–õ–¨–ù–´–ï –°–û–û–ë–©–ï–ù–ò–Ø:")
    for i, msg in enumerate(request.messages):
        logger.debug(f"    –°–æ–æ–±—â–µ–Ω–∏–µ {i+1}:")
        logger.debug(f"      Role: {msg.role}")
        logger.debug(f"      Content: {msg.content[:200]}{'...' if len(msg.content) > 200 else ''}")
        if hasattr(msg, 'tool_calls') and msg.tool_calls:
            logger.debug(f"      Tool calls: {len(msg.tool_calls)}")
        if hasattr(msg, 'tool_call_id') and msg.tool_call_id:
            logger.debug(f"      Tool call ID: {msg.tool_call_id}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Cursor
    logger.info(f"[Cursor] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∑–∞–ø—Ä–æ—Å–µ: {len(request.messages)}")
    
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        validate_chat_request(request)
        
        if getattr(request, 'stream', False):
            async def event_generator():
                async for chunk in llm_service.process_chat_request_stream(request):
                    # –§–æ—Ä–º–∏—Ä—É–µ–º OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π stream-—á–∞–Ω–∫
                    choices = []
                    for choice in chunk.choices:
                        delta = {}
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø–æ–ª—è –∏–∑ delta, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                        if "delta" in choice:
                            if "role" in choice["delta"] and choice["delta"]["role"]:
                                delta["role"] = choice["delta"]["role"]
                            if "content" in choice["delta"] and choice["delta"]["content"] is not None:
                                delta["content"] = choice["delta"]["content"]
                            if "tool_calls" in choice["delta"] and choice["delta"]["tool_calls"]:
                                delta["tool_calls"] = choice["delta"]["tool_calls"]
                        
                        choices.append({
                            "delta": delta,
                            "index": choice["index"],
                            "finish_reason": choice.get("finish_reason")
                        })
                    
                    stream_chunk = {
                        "id": chunk.id,
                        "object": "chat.completion.chunk",
                        "created": chunk.created,
                        "model": chunk.model,
                        "choices": choices
                    }
                    
                    yield f"data: {json.dumps(stream_chunk, ensure_ascii=False)}\n\n"
                    await asyncio.sleep(0)  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ [DONE]
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(event_generator(), media_type="text/event-stream")
        
        # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
        response = await llm_service.process_chat_request(request)
        
        duration = time.time() - start_time
        logger.debug("‚ú® –ó–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω", extra={
            "session_id": request.session_id,
            "total_duration_ms": round(duration * 1000, 2),
            "response_choices": len(response.choices),
            "response_id": response.id
        })
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        logger.debug("üì§ RESPONSE TO CLIENT: %s", response.json())
        print("üì§ RESPONSE TO CLIENT:", response.json())
        
        return response
        
    except ValidationError as e:
        logger.warning(f"‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except PIIProcessingError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PII: {str(e)}")
        raise HTTPException(status_code=500, detail="PII processing error")
    except LLMProviderError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: {str(e)}")
        raise HTTPException(status_code=502, detail="LLM provider error")
    except ConfigurationError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {str(e)}")
        raise HTTPException(status_code=500, detail="Configuration error")
    except Exception as e:
        duration = time.time() - start_time
        logger.error("üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞", extra={
            "session_id": request.session_id,
            "error": str(e),
            "error_type": type(e).__name__,
            "duration_ms": round(duration * 1000, 2)
        })
        raise HTTPException(status_code=500, detail="Internal server error")

@router.get("/v1/models")
async def list_models():
    """OpenAI-compatible endpoint for model listing."""
    return {
        "object": "list",
        "data": [
            {
                "id": "gpt-4.1",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "azure-openai",
                "permission": [],
                "capabilities": {
                    "tools": True,
                    "tool_choice": True,
                    "functions": True,
                    "function_calling": True,
                    "streaming": True
                }
            },
            {
                "id": "pii_llm_v1",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "pii-proxy",
                "permission": [],
                "capabilities": {
                    "tools": True,
                    "tool_choice": True,
                    "functions": True,
                    "function_calling": True,
                    "streaming": True
                }
            }
        ]
    } 